{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f03db12",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135430fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from iqual import  tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eefba0",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285f4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir         = \"../data\"\n",
    "\n",
    "### Enhanced qualitative data\n",
    "bootstrap_df = pd.read_csv(os.path.join(data_dir,\"kfold_enh_pred_mean.csv\"))\n",
    "\n",
    "### Quantitative data\n",
    "quant_df     = pd.read_csv(os.path.join(data_dir,\"quant_data.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7b858",
   "metadata": {},
   "source": [
    "### Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443cd6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_df         = pd.merge(bootstrap_df,quant_df,on=['uid','data_round','refugee'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb722f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_df.loc[:,'round_R3'] = 0\n",
    "bootstrap_df.loc[bootstrap_df.data_round=='R3','round_R3'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de75124",
   "metadata": {},
   "source": [
    "> ### Select variables of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ddf1950",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vars = [  'round_R3',       # Dummy variable for R3\n",
    "                    'refugee',       # Refugee\n",
    "                    'hh_head_sex',   # Female HH Head\n",
    "                    'eld_sex',       # Female eldest child\n",
    "                    'parent_reledu', # Religiously educated parent                    \n",
    "                    'num_child', # Number of Children\n",
    "                    'hh_head_age', # Age of HH Head\n",
    "                    'parent_eduyears', # Parent's years of education\n",
    "                    'eld_age',       # Age of eldest child\n",
    "                    'hh_asset_index', # HH asset index\n",
    "                    'hh_income',      # HH Income\n",
    "                    'int_trauma_exp', # Trauma Experience\n",
    "]\n",
    "\n",
    "annotation_vars = [\n",
    "     \"ability_high\",\n",
    "     \"ability_low\",\n",
    "     \"awareness_information_high\",\n",
    "     \"awareness_information_low\",\n",
    "     \"budget_high\",\n",
    "     \"budget_low\",\n",
    "     \"covid_impacts\",\n",
    "     \"education_high\",\n",
    "     \"education_low\",\n",
    "     \"education_neutral\",\n",
    "     \"education_religious\",\n",
    "     \"entrepreneur\",\n",
    "     \"job_secular\",\n",
    "     \"marriage\",\n",
    "     \"migration\",\n",
    "     \"no_ambition\",\n",
    "     \"public_assistance\",\n",
    "     \"reliance_on_god\",\n",
    "     \"religious\",\n",
    "     \"secular\",\n",
    "     \"vague_job\",\n",
    "     \"vague_non_specific\",\n",
    "     \"vocational_training\",\n",
    "     \"worries_anxieties\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1323f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_interpreter = \n",
    "unannotated_interpreter = tests.Interpretability(bootstrap_df[bootstrap_df.annotated==0],\n",
    "                       annotation_vars=annotation_vars,\n",
    "                       numerical_regressors=numerical_vars,\n",
    "                      ).fit_all()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e0d1a71",
   "metadata": {},
   "source": [
    "se_list = []\n",
    "for annotation_var in annotation_vars:\n",
    "    for quant_var in numerical_vars:\n",
    "        \n",
    "        interpreter = tests.Interpretability(bootstrap_df[bootstrap_df.annotated==1],\n",
    "                       annotation_vars=annotation_vars,\n",
    "                       numerical_regressors=numerical_vars,\n",
    "                      ).fit_all()        \n",
    "        \n",
    "        se_list.append({'annotation':annotation_var,'variable':quant_var,\n",
    "                        'std_err':interpreter.get_model_std_error(annotation_var,quant_var),\n",
    "                        'type':'annotated'})\n",
    "        \n",
    "        interpreter = tests.Interpretability(bootstrap_df[bootstrap_df.annotated==1],\n",
    "                       annotation_vars=annotation_vars,\n",
    "                       numerical_regressors=numerical_vars,\n",
    "                      ).fit_all()        \n",
    "\n",
    "        se_list.append({'annotation':annotation_var,'variable':quant_var,\n",
    "                        'std_err':interpreter.get_model_std_error(annotation_var,quant_var),\n",
    "                        'type':'unannotated'})        \n",
    "        \n",
    "        interpreter = tests.Interpretability(bootstrap_df,\n",
    "                       annotation_vars=annotation_vars,\n",
    "                       numerical_regressors=numerical_vars,\n",
    "                      ).fit_all()        \n",
    "        se_list.append({'annotation':annotation_var,'variable':quant_var,\n",
    "                        'std_err':interpreter.get_model_std_error(annotation_var,quant_var),\n",
    "                        'type':'enhanced',\n",
    "                       })        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean in human sample\n",
    "Mean in machine sample\n",
    "Variance of observed annotations in data\n",
    "Variance of bootstrapped residuals\n",
    "Variance of bootstrapped predictions\n",
    "Standard error in human sample\n",
    "Standard error in machine sample\n",
    "Standard error in enhanced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for annot in qual_vars:\n",
    "    row   = compute_se(enh_df, annot, use_oob=False, dec_place=4)\n",
    "    eff_df = pd.concat([eff_df, row])\n",
    "eff_df = eff_df[[\"annot\", \"sig2_h\", \"sig2_m\", \"sig2_eps\", \"se_h\", \"se_enh\"]]\n",
    "stargazer(as .matrix(eff_df), title=\"Measurement error variances\",\n",
    "          table.placement=\"H\", label=\"tab:error_variances\")\n",
    "\n",
    "# Refugee versus host\n",
    "# Refugees\n",
    "input_df = filter(enh_df, refugee == 1)\n",
    "ref_eff_df = pd.DataFrame()\n",
    "for annot in qual_vars:\n",
    "  row = compute_se(input_df, annot, use_oob=False, dec_place=4)\n",
    "  ref_eff_df = pd.concat([ref_eff_df, row])\n",
    "mean(input_df$ability_high, na.rm=T)\n",
    "# Hosts\n",
    "input_df = filter(enh_df, refugee == 0)\n",
    "host_eff_df = pd.DataFrame()\n",
    "for annot in qual_vars:\n",
    "  row = compute_se(input_df, annot, use_oob=False, dec_place=4)\n",
    "  host_eff_df = pd.concat([host_eff_df, row])\n",
    "mean(input_df$ability_high, na.rm=T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27d69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "821af8a3",
   "metadata": {},
   "source": [
    "> ### Bias tests (For each bootstrap-run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for b, boot_df in data.groupby('bootstrap_run'):\n",
    "    bias = tests.Bias(boot_df,\n",
    "                      annotation_vars=annotation_vars,\n",
    "                      categorical_regressors=categorical_vars,\n",
    "                      numerical_regressors=numerical_vars,\n",
    "                     )\n",
    "    bias.fit_all()\n",
    "    datasets.extend([{\n",
    "        'bootstrap_run':b,\n",
    "        'annotation':annotation,\n",
    "        'fstat_enh':result.fvalue,\n",
    "        'log_fstat_enh':np.log(result.fvalue),\n",
    "        'pval_enh':result.f_pvalue,\n",
    "    }   for annotation, result in bias.model_fits.items()])\n",
    "    \n",
    "bias_df = pd.DataFrame(datasets)    \n",
    "bias_df = bias_df.sort_values('annotation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc4b38",
   "metadata": {},
   "source": [
    "> ### Significance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_df['sig_level'] = '>5%'\n",
    "bias_df.loc[bias_df.pval_enh <= 0.01, 'sig_level'] = '<1%'\n",
    "bias_df.loc[(bias_df.pval_enh > 0.01) & (bias_df.pval_enh <= 0.05), 'sig_level'] = '<5%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a1ef2",
   "metadata": {},
   "source": [
    "> ### Bias test (Average across Bootstrap runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffdb9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = data.drop(columns=['bootstrap_run']).groupby(['uid','data_round','refugee'])\n",
    "avg_data   = grouped_df.mean(numeric_only=False).reset_index()\n",
    "\n",
    "bias = tests.Bias(avg_data,\n",
    "                  annotation_vars=annotation_vars,\n",
    "                  numerical_regressors=numerical_vars,\n",
    "                  categorical_regressors=categorical_vars,\n",
    "                 )\n",
    "bias.fit_all()\n",
    "\n",
    "bias_avg_df = pd.DataFrame([{\n",
    "        'annotation':annotation,\n",
    "        'fstat_enh':result.fvalue,\n",
    "        'log_fstat_enh':np.log(result.fvalue),\n",
    "        'pval_enh':result.f_pvalue,\n",
    "    }   for annotation, result in bias.model_fits.items()])\n",
    "\n",
    "bias_avg_df = bias_avg_df.sort_values('annotation',ascending=True,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856526d",
   "metadata": {},
   "source": [
    "> ### Significance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bab5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_avg_df['sig_level'] = '>5%'\n",
    "bias_avg_df.loc[bias_avg_df.pval_enh <= 0.01, 'sig_level'] = '<1%'\n",
    "bias_avg_df.loc[(bias_avg_df.pval_enh > 0.01) & (bias_avg_df.pval_enh <= 0.05), 'sig_level'] = '<5%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1170bfd",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a93bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "#### Colors for p-value\n",
    "pvalue_color_dict = {'>5%':'lightgreen','<5%':'yellow','<1%':'red'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "#### Bias (Bootstrap)\n",
    "ax.scatter(bias_df['log_fstat_enh'], bias_df['annotation'], \n",
    "           s=100, \n",
    "           color=(1,1,1,0.5),\n",
    "           lw=2,\n",
    "           edgecolors=bias_df['sig_level'].replace(pvalue_color_dict),\n",
    "          )\n",
    "\n",
    "#### Bias (Average)\n",
    "ax.scatter(bias_avg_df['log_fstat_enh'], bias_avg_df['annotation'], \n",
    "           s=100, \n",
    "           lw=2,\n",
    "           c=bias_avg_df['sig_level'].replace(pvalue_color_dict),\n",
    "           edgecolors='k',\n",
    "          )\n",
    "\n",
    "\n",
    "# Legend\n",
    "\n",
    "# 1. Legend for p-value\n",
    "pval_patches    = [\n",
    "    Line2D([0], [0], \n",
    "           marker='.', c=color, lw=0,ms=10,\n",
    "           label=significance,\n",
    "          )\n",
    "     for significance, color in pvalue_color_dict.items() ]\n",
    "\n",
    "first_legend = plt.legend(handles=pval_patches,title='p-value',loc='lower left')\n",
    "plt.gca().add_artist(first_legend)\n",
    "\n",
    "# 2. Legend for Mean/Bootsrap\n",
    "marker_patches =  [\n",
    "    Line2D([0], [0], marker='.', c='k', lw=0, ms=20, label=\"Bootstrap\"),\n",
    "    Line2D([0], [0], marker='o', c='k', lw=0,  ms=10, mfc='w', label=\"Mean\")\n",
    "]\n",
    "\n",
    "second_legend = plt.legend(handles=marker_patches, loc='upper left')\n",
    "plt.gca().add_artist(second_legend)\n",
    "\n",
    "# Axes\n",
    "ax.grid(True,color='lightgray',)\n",
    "ax.set_xlabel('log F statistic',fontsize=12)\n",
    "ax.set_ylabel('Annotation', fontsize=12)\n",
    "ax.set_title('Bias test for each annotation',fontsize=18, pad=25)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61615438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
